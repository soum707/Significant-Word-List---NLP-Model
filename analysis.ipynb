{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895d47f7",
   "metadata": {},
   "source": [
    "Tasks: <br>\n",
    "<br>\n",
    "1- get the full lemmatized word list <br>\n",
    "2- get the 95% coverage word list <br>\n",
    "3- remove stop words from 95% swl <br>\n",
    "4- compare ngsl coverage on full lemmatized word list\n",
    "\n",
    "- make 95% default and changable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a80597",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da386979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/soum/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/soum/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/soum/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/soum/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/soum/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a30fad88d049ac9a62b7a8785dbaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 18:32:09 INFO: Downloaded file to /Users/soum/stanza_resources/resources.json\n",
      "2025-04-25 18:32:09 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-04-25 18:32:11 INFO: File exists: /Users/soum/stanza_resources/en/default.zip\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import stanza\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stanza.download('en')\n",
    "nlp1 = spacy.load(\"en_core_web_sm\")\n",
    "nlp2 = stanza.Pipeline(lang='en', processors='tokenize,pos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b0542",
   "metadata": {},
   "source": [
    "### Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        src = file.read()\n",
    "        src = src.replace(\"\\n\", \" \")\n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45dd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = [[j.lower() for j in word_tokenize(i)] for i in sent_tokenize(text)]\n",
    "\n",
    "    words = []\n",
    "    for sentence in tokens:\n",
    "        words.extend(sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88709f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequncy_df(words):\n",
    "    text_counts = pd.DataFrame({'word': words})\n",
    "    text_counts = text_counts.groupby('word')['word'].count().reset_index(name='count')\n",
    "    text_counts = text_counts.sort_values(by='count', ascending=False)\n",
    "    text_counts = text_counts[text_counts['word'].str.isalpha()].reset_index(drop=True)\n",
    "    return text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df):\n",
    "    lemma_freq = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['word']\n",
    "        count = row['count']\n",
    "        doc = nlp1(word)\n",
    "        token = doc[0]\n",
    "        if token.like_num:\n",
    "            continue\n",
    "        lemma = token.lemma_\n",
    "        lemma_freq[lemma] = lemma_freq.get(lemma, 0) + count\n",
    "\n",
    "    grouped_df = pd.DataFrame(list(lemma_freq.items()), columns=['word', 'count'])\n",
    "    grouped_df = grouped_df.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swl(df, coverage: float = 0.95):\n",
    "    if not 0.0 <= coverage <= 1.0:\n",
    "        raise ValueError(\"`coverage` must be between 0.0 and 1.0\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['cumulative_coverage'] = df['count'].cumsum() / df['count'].sum()\n",
    "    swl = df[df['cumulative_coverage'] <= coverage]\n",
    "    return swl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c51174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    mod_text = (\n",
    "        text[~text['word'].isin(stop_words)]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return mod_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575bbec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_proper_nouns(df):\n",
    "    drops = []\n",
    "    for i, txt in df['word'].items():\n",
    "        doc = nlp2(txt)\n",
    "        if any(word.upos == 'PROPN' for sent in doc.sentences for word in sent.words):\n",
    "            drops.append(i)\n",
    "    return df.drop(drops).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fca1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(new_series, original_series):\n",
    "    new_sum = new_series.sum()\n",
    "    original_sum = original_series.sum()\n",
    "    coverage = (new_sum / original_sum) * 100\n",
    "    return print(\"Coverage: \", coverage, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ba3757",
   "metadata": {},
   "source": [
    "### Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57adfb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_origianl_swl(file_path):\n",
    "    original_df = load_text_file(file_path)\n",
    "    original_df = tokenize_text(original_df)\n",
    "    original_df = generate_frequncy_df(original_df)\n",
    "\n",
    "    return original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ef050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatized_swl(file_path):\n",
    "    # Load the text file\n",
    "    text = load_text_file(file_path)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    # Generate frequency DataFrame\n",
    "    freq_df = generate_frequncy_df(tokens)\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatized_df = lemmatize(freq_df)\n",
    "    \n",
    "    return lemmatized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_95_swl(lemmatized_df, \n",
    "               remove_sw: bool = False, \n",
    "               remove_pn: bool = False):\n",
    "    \n",
    "    # Get the 95% SWL\n",
    "    swl = get_swl(lemmatized_df)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_sw:\n",
    "        swl = remove_stopwords(swl)\n",
    "\n",
    "    # Remove proper nouns\n",
    "    if remove_pn:\n",
    "        swl = remove_proper_nouns(swl)\n",
    "    \n",
    "    return swl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ngsl_lemmatized(lemmatized_df):\n",
    "    ngsl = pd.read_csv(r\"data/ngsl-v1.2.csv\")\n",
    "    ngsl.rename(columns={'Adjusted Frequency per Million (U)': 'count', 'Lemma': 'word'}, inplace=True)\n",
    "\n",
    "    common_df = pd.merge(\n",
    "    ngsl, lemmatized_df,\n",
    "    on='word',\n",
    "    how='inner',\n",
    "    suffixes=('_ngsl', '_df')\n",
    "    )\n",
    "\n",
    "    coverage = float((common_df['count_df'].sum() / lemmatized_df['count'].sum())* 100)\n",
    "    print(f\"Coverage of NGSL in lemmatized SWL: {coverage:.2f}%\")\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_nawl_lemmatized(lemmatized_df):\n",
    "    nawl = pd.read_csv(r\"data/NAWL-1.0.csv\")\n",
    "    nawl.rename(columns={'Word': 'word', 'U': 'count'}, inplace=True)\n",
    "\n",
    "    common_df = pd.merge(\n",
    "    nawl, lemmatized_df,\n",
    "    on='word',\n",
    "    how='inner',\n",
    "    suffixes=('_nawl', '_df')\n",
    "    )\n",
    "\n",
    "    coverage = float((common_df['count_df'].sum() / lemmatized_df['count'].sum())* 100)\n",
    "    print(f\"Coverage of NAWL in lemmatized SWL: {coverage:.2f}%\")\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c817a34",
   "metadata": {},
   "source": [
    "### Text Data Management and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhai_df = load_text_file(r\"data/zhai.txt\")\n",
    "\n",
    "zhai_tokens = tokenize_text(zhai_df)\n",
    "\n",
    "zhai_tokens = generate_frequncy_df(zhai_tokens)\n",
    "\n",
    "len(zhai_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f03546",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhai_df = get_lemmatized_swl(r\"data/zhai.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0524fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(zhai_df))\n",
    "zhai_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cf8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhai_swl = get_95_swl(zhai_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(zhai_swl))\n",
    "zhai_swl.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhai_swl_no_sw = get_95_swl(zhai_df, remove_sw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c8ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(zhai_swl_no_sw))\n",
    "zhai_swl_no_sw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhai_nawl_coverage = compare_ngsl_lemmatized(zhai_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhai_swl_coverage = coverage(zhai_swl['count'], zhai_df['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fab872",
   "metadata": {},
   "source": [
    "### Alice in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_df = load_text_file(r\"data/alice.txt\")\n",
    "\n",
    "alice_tokens = tokenize_text(alice_df)\n",
    "\n",
    "alice_tokens = generate_frequncy_df(alice_tokens)\n",
    "\n",
    "len(alice_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_df = get_lemmatized_swl(r\"data/alice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963daee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(alice_df))\n",
    "alice_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8de98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_swl = get_95_swl(alice_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349498a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(alice_swl))\n",
    "alice_swl.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_swl_no_sw = get_95_swl(alice_df, remove_sw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35dd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(alice_swl_no_sw))\n",
    "alice_swl_no_sw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_ngsl_coverage = compare_ngsl_lemmatized(alice_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_swl_coverage = coverage(alice_swl['count'], alice_df['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e677088",
   "metadata": {},
   "source": [
    "### Titanic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236abd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = load_text_file(r\"data/titanic.txt\")\n",
    "titanic_df = tokenize_text(titanic_df)\n",
    "titanic_df = generate_frequncy_df(titanic_df)\n",
    "print(len(titanic_df))\n",
    "titanic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee83497",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = get_95_swl(titanic_df, remove_sw=True)\n",
    "print(len(titanic_df))\n",
    "titanic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_lemmatized_df = get_lemmatized_swl(r\"data/titanic.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(titanic_lemmatized_df))\n",
    "print(titanic_lemmatized_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926bbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_swl = get_95_swl(titanic_lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf629d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(titanic_swl))\n",
    "print(titanic_swl.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_swl_nosw = get_95_swl(titanic_lemmatized_df, remove_sw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(titanic_swl_nosw))\n",
    "print(titanic_swl_nosw.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ngsl_coverage = compare_ngsl_lemmatized(titanic_lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_swl_coverage = coverage(titanic_swl['count'], titanic_lemmatized_df['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b244578",
   "metadata": {},
   "source": [
    "### Lord of the Rings Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ef0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_lemmatized_df = get_lemmatized_swl(r\"data/Lord of the Rings - Chapter One.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lotr_lemmatized_df))\n",
    "print(lotr_lemmatized_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_swl = get_95_swl(lotr_lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd009e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lotr_swl))\n",
    "print(lotr_swl.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_swl_nosw = get_95_swl(lotr_lemmatized_df, remove_sw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e86b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lotr_swl_nosw))\n",
    "print(lotr_swl_nosw.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09cc190",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_compare_ngsl_coverage = compare_ngsl_lemmatized(lotr_lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_swl_coverage = coverage(lotr_swl['count'], lotr_lemmatized_df['count'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

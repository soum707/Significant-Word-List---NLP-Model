{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895d47f7",
   "metadata": {},
   "source": [
    "Tasks: <br>\n",
    "<br>\n",
    "1- get the full lemmatized word list <br>\n",
    "2- get the 95% coverage word list <br>\n",
    "3- remove stop words from 95% swl <br>\n",
    "4- compare ngsl coverage on full lemmatized word list\n",
    "\n",
    "- make 95% default and changable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a80597",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da386979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\semse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\semse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\semse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\semse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\semse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19a835b10a14582b5f39a85a1200157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:31:54 INFO: Downloaded file to C:\\Users\\semse\\stanza_resources\\resources.json\n",
      "2025-04-24 20:31:54 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-04-24 20:31:55 INFO: File exists: C:\\Users\\semse\\stanza_resources\\en\\default.zip\n",
      "2025-04-24 20:31:58 INFO: Finished downloading models and saved to C:\\Users\\semse\\stanza_resources\n",
      "2025-04-24 20:31:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3509bb78806d45f3a7b12c4208b40c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:31:59 INFO: Downloaded file to C:\\Users\\semse\\stanza_resources\\resources.json\n",
      "2025-04-24 20:31:59 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-04-24 20:32:00 INFO: Loading these models for language: en (English):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2025-04-24 20:32:00 INFO: Using device: cpu\n",
      "2025-04-24 20:32:00 INFO: Loading: tokenize\n",
      "2025-04-24 20:32:00 INFO: Loading: mwt\n",
      "2025-04-24 20:32:00 INFO: Loading: pos\n",
      "2025-04-24 20:32:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import stanza\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stanza.download('en')\n",
    "nlp1 = spacy.load(\"en_core_web_sm\")\n",
    "nlp2 = stanza.Pipeline(lang='en', processors='tokenize,pos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b0542",
   "metadata": {},
   "source": [
    "### Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64fa5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        src = file.read()\n",
    "        src = src.replace(\"\\n\", \" \")\n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c45dd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = [[j.lower() for j in word_tokenize(i)] for i in sent_tokenize(text)]\n",
    "\n",
    "    words = []\n",
    "    for sentence in tokens:\n",
    "        words.extend(sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a88709f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequncy_df(words):\n",
    "    text_counts = pd.DataFrame({'word': words})\n",
    "    text_counts = text_counts.groupby('word')['word'].count().reset_index(name='count')\n",
    "    text_counts = text_counts.sort_values(by='count', ascending=False)\n",
    "    text_counts = text_counts[text_counts['word'].str.isalpha()].reset_index(drop=True)\n",
    "    return text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3d277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df):\n",
    "    lemma_freq = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['word']\n",
    "        count = row['count']\n",
    "        doc = nlp1(word)\n",
    "        token = doc[0]\n",
    "        if token.like_num:\n",
    "            continue\n",
    "        lemma = token.lemma_\n",
    "        lemma_freq[lemma] = lemma_freq.get(lemma, 0) + count\n",
    "\n",
    "    grouped_df = pd.DataFrame(list(lemma_freq.items()), columns=['word', 'count'])\n",
    "    grouped_df = grouped_df.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b5d8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swl(df, coverage: float = 0.95):\n",
    "    if not 0.0 <= coverage <= 1.0:\n",
    "        raise ValueError(\"`coverage` must be between 0.0 and 1.0\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['cumulative_coverage'] = df['count'].cumsum() / df['count'].sum()\n",
    "    swl = df[df['cumulative_coverage'] <= coverage]\n",
    "    return swl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31c51174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    mod_text = (\n",
    "        text[~text['word'].isin(stop_words)]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return mod_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575bbec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_proper_nouns(df):\n",
    "    drops = []\n",
    "    for i, txt in df['word'].items():\n",
    "        doc = nlp2(txt)\n",
    "        if any(word.upos == 'PROPN' for sent in doc.sentences for word in sent.words):\n",
    "            drops.append(i)\n",
    "    return df.drop(drops).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79fca1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(new_series, original_series):\n",
    "    new_sum = new_series.sum()\n",
    "    original_sum = original_series.sum()\n",
    "    coverage = (new_sum / original_sum) * 100\n",
    "    return print(\"Coverage: \", coverage, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ba3757",
   "metadata": {},
   "source": [
    "### Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "237ef050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatized_swl(file_path):\n",
    "    # Load the text file\n",
    "    text = load_text_file(file_path)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    # Generate frequency DataFrame\n",
    "    freq_df = generate_frequncy_df(tokens)\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatized_df = lemmatize(freq_df)\n",
    "    \n",
    "    return lemmatized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c09e4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_95_swl(lemmatized_df, \n",
    "               remove_sw: bool = False, \n",
    "               remove_pn: bool = False):\n",
    "    \n",
    "    # Get the 95% SWL\n",
    "    swl = get_swl(lemmatized_df)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_sw:\n",
    "        swl = remove_stopwords(swl)\n",
    "\n",
    "    # Remove proper nouns\n",
    "    if remove_pn:\n",
    "        swl = remove_proper_nouns(swl)\n",
    "    \n",
    "    return swl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f85bcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ngsl_lemmatized(lemmatized_df):\n",
    "    ngsl = pd.read_csv(\"ngsl-v1.2.csv\")\n",
    "    ngsl.rename(columns={'Adjusted Frequency per Million (U)': 'count', 'Lemma': 'word'}, inplace=True)\n",
    "\n",
    "    common_df = pd.merge(\n",
    "    ngsl, lemmatized_df,\n",
    "    on='word',\n",
    "    how='inner',\n",
    "    suffixes=('_ngsl', '_df')\n",
    "    )\n",
    "\n",
    "    # df1_filtered = df1[df1['word'].isin(df2['word'])].reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    coverage = float((common_df['count_df'].sum() / lemmatized_df['count'].sum())* 100)\n",
    "    print(f\"Coverage of NGSL in lemmatized SWL: {coverage:.2f}%\")\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e677088",
   "metadata": {},
   "source": [
    "### Titanic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "236abd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5341\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>3013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>1102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rose</td>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jack</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  count\n",
       "0   the   3013\n",
       "1   and   1146\n",
       "2    to   1102\n",
       "3     a   1049\n",
       "4    of    840\n",
       "5    in    665\n",
       "6  rose    664\n",
       "7    is    602\n",
       "8    it    556\n",
       "9  jack    529"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = load_text_file(\"titanic.txt\")\n",
    "titanic_df = tokenize_text(titanic_df)\n",
    "titanic_df = generate_frequncy_df(titanic_df)\n",
    "print(len(titanic_df))\n",
    "titanic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eee83497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>cumulative_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rose</td>\n",
       "      <td>664</td>\n",
       "      <td>0.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jack</td>\n",
       "      <td>529</td>\n",
       "      <td>0.237874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cal</td>\n",
       "      <td>227</td>\n",
       "      <td>0.344619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cut</td>\n",
       "      <td>200</td>\n",
       "      <td>0.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>water</td>\n",
       "      <td>183</td>\n",
       "      <td>0.372090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deck</td>\n",
       "      <td>182</td>\n",
       "      <td>0.376348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>like</td>\n",
       "      <td>179</td>\n",
       "      <td>0.388937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>back</td>\n",
       "      <td>163</td>\n",
       "      <td>0.409060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boat</td>\n",
       "      <td>140</td>\n",
       "      <td>0.415635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ship</td>\n",
       "      <td>127</td>\n",
       "      <td>0.421883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  count  cumulative_coverage\n",
       "0   rose    664             0.198400\n",
       "1   jack    529             0.237874\n",
       "2    cal    227             0.344619\n",
       "3    cut    200             0.354400\n",
       "4  water    183             0.372090\n",
       "5   deck    182             0.376348\n",
       "6   like    179             0.388937\n",
       "7   back    163             0.409060\n",
       "8   boat    140             0.415635\n",
       "9   ship    127             0.421883"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = get_95_swl(titanic_df, remove_sw=True)\n",
    "print(len(titanic_df))\n",
    "titanic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f7a3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_lemmatized_df = get_lemmatized_swl(\"titanic.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b66ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4050\n",
      "   word  count\n",
      "0   the   3013\n",
      "1   and   1146\n",
      "2    to   1102\n",
      "3    be   1099\n",
      "4     a   1049\n",
      "5   she    899\n",
      "6    of    840\n",
      "7  rise    683\n",
      "8    in    665\n",
      "9    he    592\n"
     ]
    }
   ],
   "source": [
    "print(len(titanic_lemmatized_df))\n",
    "print(titanic_lemmatized_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "926bbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_swl = get_95_swl(titanic_lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcf629d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049\n",
      "   word  count  cumulative_coverage\n",
      "0   the   3013             0.071261\n",
      "1   and   1146             0.098366\n",
      "2    to   1102             0.124429\n",
      "3    be   1099             0.150422\n",
      "4     a   1049             0.175232\n",
      "5   she    899             0.196495\n",
      "6    of    840             0.216362\n",
      "7  rise    683             0.232516\n",
      "8    in    665             0.248244\n",
      "9    he    592             0.262245\n"
     ]
    }
   ],
   "source": [
    "print(len(titanic_swl))\n",
    "print(titanic_swl.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ec1a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_swl_nosw = get_95_swl(titanic_lemmatized_df, remove_sw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f214da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1946\n",
      "   word  count  cumulative_coverage\n",
      "0  rise    683             0.232516\n",
      "1  jack    529             0.287907\n",
      "2     I    519             0.300182\n",
      "3    go    232             0.376954\n",
      "4   cal    227             0.382323\n",
      "5   cut    211             0.397720\n",
      "6   see    205             0.402569\n",
      "7  deck    192             0.411793\n",
      "8  look    187             0.420662\n",
      "9  boat    184             0.425014\n"
     ]
    }
   ],
   "source": [
    "print(len(titanic_swl_nosw))\n",
    "print(titanic_swl_nosw.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f9fade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of NGSL in lemmatized SWL: 79.36%\n"
     ]
    }
   ],
   "source": [
    "titanic_ngsl_coverage = compare_ngsl_lemmatized(titanic_lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0baa500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage:  94.9977531278825 %\n"
     ]
    }
   ],
   "source": [
    "titanic_swl_coverage = coverage(titanic_swl['count'], titanic_lemmatized_df['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b244578",
   "metadata": {},
   "source": [
    "### Lord of the Rings Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "151ef0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_lemmatized_df = get_lemmatized_swl(\"Lord of the Rings - Chapter One.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a2e4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394\n",
      "   word  count\n",
      "0   the    496\n",
      "1   and    379\n",
      "2    be    378\n",
      "3    of    287\n",
      "4    he    224\n",
      "5     I    210\n",
      "6    to    201\n",
      "7     a    167\n",
      "8  have    158\n",
      "9   you    154\n"
     ]
    }
   ],
   "source": [
    "print(len(lotr_lemmatized_df))\n",
    "print(lotr_lemmatized_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeb7b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_swl = get_95_swl(lotr_lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bd009e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944\n",
      "   word  count  cumulative_coverage\n",
      "0   the    496             0.055129\n",
      "1   and    379             0.097255\n",
      "2    be    378             0.139269\n",
      "3    of    287             0.171168\n",
      "4    he    224             0.196065\n",
      "5     I    210             0.219406\n",
      "6    to    201             0.241747\n",
      "7     a    167             0.260309\n",
      "8  have    158             0.277870\n",
      "9   you    154             0.294987\n"
     ]
    }
   ],
   "source": [
    "print(len(lotr_swl))\n",
    "print(lotr_swl.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bee95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_swl_nosw = get_95_swl(lotr_lemmatized_df, remove_sw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01e86b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837\n",
      "      word  count  cumulative_coverage\n",
      "0        I    210             0.219406\n",
      "1      say     94             0.368901\n",
      "2    frodo     92             0.379126\n",
      "3     come     42             0.445148\n",
      "4      see     38             0.453929\n",
      "5  gandalf     36             0.462043\n",
      "6    think     36             0.466044\n",
      "7    bilbo     36             0.470046\n",
      "8     many     34             0.473825\n",
      "9     look     33             0.481272\n"
     ]
    }
   ],
   "source": [
    "print(len(lotr_swl_nosw))\n",
    "print(lotr_swl_nosw.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b09cc190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of NGSL in lemmatized SWL: 83.83%\n"
     ]
    }
   ],
   "source": [
    "lotr_compare_ngsl_coverage = compare_ngsl_lemmatized(lotr_lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2287ef60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage:  94.99833277759254 %\n"
     ]
    }
   ],
   "source": [
    "lotr_swl_coverage = coverage(lotr_swl['count'], lotr_lemmatized_df['count'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
